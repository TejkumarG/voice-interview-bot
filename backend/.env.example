# ==================== API KEYS ====================

# OpenAI API Key (get from: https://platform.openai.com/api-keys)
# Used for both embeddings and LLM (GPT-4o-mini)
OPENAI_API_KEY=sk-your_openai_api_key_here

# Pinecone API Key (get from: https://app.pinecone.io)
PINECONE_API_KEY=your_pinecone_api_key_here


# ==================== PINECONE CONFIGURATION ====================

# Pinecone environment (default: gcp-starter for free tier)
PINECONE_ENVIRONMENT=gcp-starter

# Pinecone index name (will be created automatically if doesn't exist)
PINECONE_INDEX_NAME=interview-bot


# ==================== CORS CONFIGURATION ====================

# Allowed frontend origins (comma-separated)
# For local development:
CORS_ORIGINS=http://localhost:3000

# For production, add your Vercel URL:
# CORS_ORIGINS=http://localhost:3000,https://your-app.vercel.app


# ==================== DOCUMENT PROCESSING ====================

# Characters per chunk when splitting documents
CHUNK_SIZE=2000

# Overlapping characters between chunks (maintains context)
CHUNK_OVERLAP=300

# Maximum file size in bytes (5MB = 5 * 1024 * 1024)
MAX_FILE_SIZE=5242880


# ==================== MODEL CONFIGURATION ====================

# OpenAI embedding model (text-embedding-3-small is cheapest & fastest)
EMBEDDING_MODEL=text-embedding-3-small

# OpenAI LLM model (gpt-4o-mini is fast, affordable & high quality)
LLM_MODEL=gpt-4o-mini

# Maximum tokens in LLM response
MAX_TOKENS=500

# Temperature for response generation (0.0 = deterministic, 1.0 = creative)
TEMPERATURE=0.7
